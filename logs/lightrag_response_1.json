{
    "success": true,
    "result": {
        "query": "什么是RAG",
        "chunks": [
            {
                "id": "1",
                "doc_id": "unknown_source",
                "content": "forms other baselines, on PopQA or Bio, powerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from their non-retrieval baselines. However, we found that these baselines provide limited solutions for tasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth and ARC-Challenge, baselines with retrieval do not improve performance notably from their no- retrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation accuracy. On ASQA, our model shows significantly higher citation precision and recall than all models except ChatGPT. Gao et al. found that ChatGPT consistently exhibits superior efficacy in this particular task, surpassing smaller LMs. Our SELF-RAG bridges this performance gap, even outperforming ChatGPT in citation precision, which measures whether the model-generated claim is fully supported by cited evidence. We also found that on the metrics for factual precision, SELF-RAG 7B occasionally outperforms our 13B due to the tendency of smaller SELF-RAG to often generate 8 Preprint. PQA Med AS (acc) (acc) (em) SELF-RAG (50k) 45.5 73.5 32.1 Training No Retriever R 43.6 67.8 31.0 No Critic C 42.6 72.0 18.1 Test No retrieval 24.7 73.0 – Hard constraints 28.3 72.6 – Retrieve top1 41.8 73.1 28.6 Remove ISSUP 44.1 73.2 30.6 (a) Ablation 1 2 70.0 70.5 Precision 1 2 Weight for IsSupport 90 95 Mauve (b) Customization 0.0 0.2 0.4 0.6 0.98 0.99 0.99 1.00 Accuracy PubHealth 0.0 0.2 0.4 0.6 Retrieval Threshold 0.6 0.8 1.0 Accuracy PopQA 0.0 0.5 1.0 Frequency 0.25 0.50 0.75 1.00 Frequency (c) Retrieval Figure 3: Analysis on SELF-RAG: (a) Ablation studies for key components of SELF-RAG training and inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and Mauve (fluency). (c) Retrieval frequency and normalized accuracy on PubHealth and PopQA. precisely grounded yet shorter outputs. Llama2-FT7B, which is the baseline LM trained on the same instruction-output pairs as SELF-RAG without retrieval or self-reflection and is retrieval-augmented at test time only, lags behind SELF-RAG. This result indicates SELF-RAG gains are not solely from training data and demonstrate the effectiveness of SELF-RAG framework. 5.2 ANALYSIS Ablation studies. We conduct a set of ablations of our framework to identify which factors play key roles. We evaluate two model variants trained differently than our model: No Retriever trains an LM using the standard instruction-following method given instruction-output pairs, without retrieved passages; No Critic trains an LM trained with input-output pairs that are always augmented with the top one retrieved document without reflection tokens. This is similar to SAIL , and we use our instruction-output data instead of using the Alpaca dataset , as in SAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables retrieval during inference; Hard constraints indicates the model performance that retrieves when Retrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the top one document only, similar to standard RAG approaches; Remove ISSUP indicates the model performance that removes ISSUP score only during critique-guided beam search in Eq. 4. In this ablation experiment, we use a training instance size of 50k for a more efficient exploration of training variations. Later in this section, we conduct an analysis of the effect of training data size. We conduct the ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models on sampled 150 instances and exclude ablations involving adaptive or no retrieval processes. We show in Table 3a the ablation results. The top part of the table shows results for training ablations, and the bottom part is for inference ablations. We see that all components play important roles. We also observe a large performance gap between SELF-RAG and No Retriever or Critic baselines across tasks, indicating that training an LM with those models largely contributes to the performance gain of SELF-RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional RAG approaches causes a large drop in PopQA and ASQA, and removing ISSUP during the beam search results hurts performance on ASQA. This demonstrates the effectiveness of SELF-RAG’s capabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively using all of the top passages from the retrieval model or solely depending on relevance scores. Effects of inference-time customization. One key benefit of our proposed framework is that it enables us to control how much each critique type affects the final generation sampling. We analyze the effects of different parameter weights on the top of our 7B model during inference time on ASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing the weighting term for ISSUP , which criticizes how supported the output is by the text passage. As the figure shows, increasing the weight leads to positive effects on the models’ citation precision since this puts more emphasis on whether model generation is",
                "chunk_index": 0,
                "score": null,
                "similarity": null
            },
            {
                "id": "2",
                "doc_id": "unknown_source",
                "content": "and responsibility of LLMs, M3Exam focuses on human exam and ToolBench evaluates how well LLMs use external tools. Recently, Ad- lakha et al. evaluate the RAG of LLMs in exist QA dataset. Different from their work, we focus on 4 required abilities of RAG and create Retrieval-Augmented Genera- tion Benchmark to evaluate the LLMs. Retrieval-Augmented Generation Benchmark In this section, we first introduce the specific retrieval- augmented generation abilities we aim to evaluate. Next, we outline the process of constructing the RAG benchmark for evaluation. Lastly, we present the evaluation metrics. Required abilities of RAG External knowledge is the key to resolving the problems of LLMs such as hallucination and outdated knowledge, which can make LLMs generate more accurate and reliable responses through retrieval-augmented generation (RAG). However, LLMs cannot always response as expected with RAG. For one thing, there are numerous irrelevant docu- ments and false information on the Internet. Incorporating these external documents into LLMs could have a detrimen- tal effect. For anthoer, LLMs suffer from the unreliable gen- eration challenge. The generation of LLMs is often unpre- dictable, and we cannot guarantee that they will utilize the useful information entailed in the external documents. Ad- ditionally, LLMs can easily be misled by incorrect infor- mation in the document. To this end, we build Retrieval- Augmented Generation Benchmark (RGB) to evaluate the retrieval-augmented generation of LLMs, and we concern about 4 specific abilities: Noise Robustness is the robustness of LLMs in noisy documents. As retrievers are not perfect, the external knowl- edge they retrieve often contains a significant amount of noise, i.e., documents which are relevant to the question but do not contain any information about the answer. To effec- tively answer user questions, LLMs must be able to extract the necessary information from documents despite there are noisy documents. Negative Rejection is a measure of whether LLMs can decline to answer a question when none of the contexts pro- vide useful information. In real-world situations, the search engine often fails to retrieve documents containing the an- swers. In these cases, it is important for the model to have the capability to reject recognition and avoid generating mis- leading content. Information Integration is a capacity to integrate an- swers from multiple documents. In many cases, the an- swer to a question may be contained in multiple documents. For example, for the question “Who are the champions of the U.S. Open 2022 men’s and women’s singles?”, the two champions may be mentioned in different documents. In or- der to provide better answers to complex questions, it is nec- essary for LLMs to have the ability to integrate information. Counterfactual Robustness refers to a capacity to han- dle errors in external knowledge. In the real world, there is an abundance of false information on the internet. Please note that we only evaluate the situation that LLMs are given warnings about potential risks in the retrieved information through instruction. In real-world scenarios, it is not possible to obtain per- fect documents with all the necessary external knowledge. Therefore, evaluating these four abilities of the model be- comes essential in order to measure the RAG of LLMs. Data construction Inspired by previous benchmarks for LLMs, RGB utilizes a question-answering format for evaluation. We evaluate the LLMs by judging the retrieval-augmented responses of them to the questions. To simulate real-world scenarios, we con- struct question and answer data using actual news articles. Due to the abundance of knowledge contained within the LLMs there is a potential for bias when measuring the first three abilities. To mitigate this, the instances of RGB are constructed by latest news articles. Additionally, we retrieve external documents from Internet through search engines. Finally, we expand the corpus and divided it into 4 testbeds to evaluate the above basic abilities of LLMs. The overall procedure of our data construction is illustrated in Figure 2. QA instances generation. We first collect latest news ar- ticles and use prompts to make ChatGPT generate events, questions, and answers for each articles. For example, as shown in the Figure 2, for a report about “The 2022 Nobel Prize”, ChatGPT will generate corresponding event, ques- tion and provide key information for answering it. By gen- erating events, the model is able to preliminarily filter out news articles that do not contain any events. After genera- tion, we manually check the answer and filter out data that is difficult to retrieve through search engines. Retrieve using search engine. For each query, we use Google’s API to fetch 10 relevant web pages and extract corresponding snippets of text from them. Simultaneously, we read these web pages and convert their textual content into text chunks with a maximum length of 300 tokens. Us- ing an existing dense retrieval model 2, we select the top-30 text chunks that match the query most effectively. These re- trieved text chunks, along with the snippets provided by the search API, will serve as our external documents. These doc- uments will be divided into positive documents and negative documents based on whether they contain the answer. Testbeds construction for each ability. We expand the corpus and divided it into 4 testbeds to evaluate the above basic abilities of LLMs. To evaluate the noise robustness, we sample varying numbers of negative documents ac- cording to the desired ratio of noises. For negative rejec- tion, all the external documents are sampled from negative documents. For the information integration ability, we fur- ther construct data based on the above generated questions. This involves expanding or",
                "chunk_index": 1,
                "score": null,
                "similarity": null
            },
            {
                "id": "3",
                "doc_id": "unknown_source",
                "content": "its effectiveness, ICL usually relies heavily on the quality of the provided demonstrations [143? ], which may lead to the gener- ation of sub-optimal outputs. Even worse, ICL may not have enough necessary information or prior knowledge to guide the LLMs in generating accurate responses. To address the aforementioned limi- tations of ICL, more recent studies introduce Retrieval-Augmented Generation (RAG) technologies [74, 117, 135]. By integrating re- trieval with generation, RAG models provide a promising direction for enhancing the performance and adaptability of LLMs across various tasks. 3 RETRIEVAL-AUGMENTED LARGE LANGUAGE MODELS (RA-LLMS) The RAG framework in the era of LLMs consists of several major processes: retrieval, generation, and augmentation, as well as the mechanism to determine whether the retrieval is needed. In this section, we will introduce important techniques involved in each process. 3.1 Retrieval Given the query from the input of LLMs, the retrieval process in RAG aims to provide relevant information from the external knowl- edge sources, which can be either open-sourced or closed-sourced as shown in Figure 3. The key component, retriever, as further detailed in Figure 4, consists of several procedures, functioning as a whole to measure the relevance between the query and documents in the database for effective information retrieval. The specific pipeline of the retrieval is further determined by whether the pre- and post-retrieval processes are included. In this subsection, we will introduce the major techniques involved in the retrieval of tradi- tional and LLM-based RAGs, including the retriever type, retrieval granularity, pre- and post-retrieval enhancement, and database construction. 3.1.1 Retriever Type. Retrieval methods can be generally catego- rized into two types: sparse and dense, based on the information encoding methods. Sparse retrieval is word-based and applied in text retrieval mostly, while dense retrieval embeds queries and ex- ternal knowledge into vector spaces and can applied to various data formats. As a straightforward approach, sparse retrieval, e.g., TF-IDF and BM25 [125, 142], usually relies on inverted index matching along with the raw data input. For example, many studies directly ap- ply BM25 for passage-level retrieval to facilitate their RAG [10, 57, 117, 168, 196, 197], where passages are specifically represented as a bag of words and ranked based on term and inverse docu- ment frequencies . On top of offering supplementary to en- hance the input of the generator, sparse retrieval has also been used to find demonstrations to function in in-context learning for RA-LLMs [2, 96, 126, 138, 176]. The main limitation of applying sparse retrieval in RAG is its no-training nature, which makes the retrieval performance heavily rely on the quality of the database and the query. Moreover, such fixed term-based methods only sup- port similarity-based retrieval, while cannot be adapted for other retrieval criteria possibly existing in LLM applications, such as the diversity . Dense retrieval, on the contrary, embeds the query and docu- ments into continuous vector space with certain criteria, for exam- ple, semantic similarity . Dense retrieval methods are usually trainable, therefore holding more flexibility and potential in adap- tation. As the key component of dense retriever, the embedding models have delicately different designs in existing RAG models. A simple design [62, 72, 165] is to directly use a part of the gener- ation model as the embedding layer of the retriever, which might be able to enhance the alignment between the retrieval and gen- eration processes. BERT-based backbone is widely applied in retrieval models. One common retriever design in RAG is to construct two-stream encoders with the BERT structure (one en- coder for the query and the other for the documents), which is also called bi-encoder [135, 164]. Early-stage RAG methods tend to freeze [6, 117] or partially freeze the parameters of the retriever to perform general-level relevant knowledge extraction and pay more attention to the knowledge leveraging and generator fine- tuning. Large-scale specialized pre-training further enhances RAG models to excel in more knowledge-intensive tasks. One typical success is Dense Passage Retriever (DPR) , which uses a BERT- based backbone and is pre-trained specifically for the OpenQA task with question-answer pair data. DPR has shown strong capacity as a pre-trained retriever, facilitating many RAG models to succeed in various downstream tasks [54, 74, 135, 139, 141]. It has also been regarded as the first step in the RAG paradigm for improving the performance of LLMs, which may further enhance the alignment of the embeddings between queries and relevant textual data through fine-tuning . A recent study has also discovered that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. With effective fine-tuning, bi-encoder retrievers are also applied widely in ICL-based RAG [82, 93, 101, 111, 126, 176]. Specifically, they have been more often used for sentence embedding similarity-based re- trieval, as well as for some special requirement in ICL, such as diverse example retrieval . Another stream of dense retrievers having been widely applied in RA-LLMs uses one encoder only, which may be based on Trans- former, BERT or other off-the-shelf sequence modeling backbones. These one-encoder retrievers are generally pre-trained on large- scale unaligned documents by contrastive learning , which may therefore excel for their versatility, meaning that they can transfer and generalize better to new domains or tasks",
                "chunk_index": 2,
                "score": null,
                "similarity": null
            }
        ],
        "metadata": {
            "instance": "space1",
            "mode": "global",
            "top_k": 6,
            "chunk_top_k": 3,
            "retrieved_chunks": 3,
            "structured_data": true
        },
        "total_results": 3,
        "retrieval_time": 0.5872253669949714
    },
    "error": null,
    "execution_time": 0.6046690940856934
}